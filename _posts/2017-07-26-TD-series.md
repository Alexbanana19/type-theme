---
layout: post
title: TD Series (0)
---

# From TD(0) to True On-Line Emphatic TD(lamda): A Great Journey
<div align=center>
<img src="{{ site.baseurl }}/img/2017-07-27-TD-learning/drl-landscape.jpeg" width="4000" height="1500" />
</div>
*This series assumes readers have basic knowledge about Reinforcemnet Learning, including(but not limited to) MDP, Dynamic Programming, Monte Carlo Learning and TD learning.*

## Beginning
Reinforcement Learning is a field with long and rich history, which can be traced back to 1950s [1]. It draws inspiration from areas such as optimal control, neural science as well as psychology, and has shown tremendous success when combing with deep learning approaches[2][3][4]. Families of modern Reinforcemnt Learning algorithms are capable of solving various real-world problems, and yet they can be too overwhelming and complexed for beginners to digest.

This series of blog posts focus on one of the base-stone algorithms in Reinforcement Learning--TD Learning method and its variants, providing both intuitions and mathematical analysis for readers who are still struggling in the field. From some simple algorithms, like TD(0), Q-learning and Sarsa, to the most cutting-edge research--True On-Line idea, Emphasis and Bounding Eligibility Trace, we will experinece an exciting journey and gain more insights of this subject.

Posts of this series are organized as follows. You can read in order or skip to the posts you are interested in.
- Understading TD Learning
- Eligibility Traces
- Function Approximation
- Off Policy Learning

Notice that I am also a beginner in Reinforcement Learining community and if there is any mistake in my posts feel free to contact me. Republishing the posts are welcomed but please do include the source. Thank you!

## References
