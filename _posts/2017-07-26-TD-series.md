---
layout: post
title: TD Series (0)
feature-img: "img/sample_feature_img.png"
---

# From TD(0) to True On-Line Emphatic TD(lamda): A Great Journey

<img src="{{ site.baseurl }}/img/2017-07-26-TD-starter/header.png" width="1500" height="400" />

<center><small> Figure 1: TD Learning Illustration. Pictures from David Silver's course: Introduction to Reinforcement Learning.[7]</small></center>

*This series assumes readers have basic knowledge about Reinforcemnet Learning, including(but not limited to) Markov Decision Process, Dynamic Programming, Monte Carlo Learning and TD learning.*

## Beginning
Reinforcement Learning is a field with long and rich history, which can be traced back to 1950s [1]. It draws inspiration from areas such as optimal control, neural science as well as psychology, and has shown tremendous success when combing with deep learning approaches [2].

Families of modern Reinforcemnt Learning algorithms are capable of solving various real-world problems, and yet they can be too overwhelming and complexed for beginners to digest.

<img src="{{ site.baseurl }}/img/2017-07-26-TD-starter/drl-landscape.jpeg" width="4000" height="500" />

<center><small> Figure 2: Families of Modern Reinforcement Learning algorithms. Pictures from[3].</small></center>


This series of blog posts focus on one of the base-stone algorithms in Reinforcement Learning--TD Learning method and its variants, providing both intuitions and mathematical analysis for readers who are still struggling in the field. From some simple algorithms, like TD(0), Q-learning and Sarsa [1], to the most cutting-edge research--True On-Line idea [4], Emphasis [5] and Bounding Eligibility Trace [6], we will experinece an exciting journey and gain more insights of this subject.

Posts of this series are organized as follows. You can read in order or skip to the posts you are interested in.

- Understading TD Learning I: Intuition
- Understading TD Learning II: Math
- Eligibility Traces I: Lambda_Return and Forward View
- Eligibility Traces II: TD(lamda) and Backward View
- Function Approximation I: Scaling Reinforcement Learning
- Function Approximation II: Artificial Neural Network
- Off Policy Learning: It's all about distributions
- Deadly Triad: Instability and Convergence Issue
- Emphatic-TD: Simple and Fast Technique
- True-Online TD(lambda): New Prospective
- On Generalized Bellman Equations and Temporal-Difference Learning
- Unifying Task Specification in Reinforcement Learning
- Emphatic Deep Q(lambda) Network


## References
[1] Sutton R S, Barto A G. Reinforcement learning: An introduction[M]. Cambridge: MIT press, 1998.

[2] Mnih V, Kavukcuoglu K, Silver D, et al. Human-level control through deep reinforcement learning[J]. Nature, 2015, 518(7540): 529-533.

[3] Zhu X, Github repository: tigerneil/awesome-deep-rl. https://github.com/tigerneil/awesome-deep-rl

[4] Seijen H, Sutton R. True online TD (lambda)[C]//International Conference on Machine Learning. 2014: 692-700.

[5] Mahmood A R, Yu H, White M, et al. Emphatic temporal-difference learning[J]. arXiv preprint arXiv:1507.01569, 2015.

[6] Yu H, Mahmood A R, Sutton R S. On Generalized Bellman Equations and Temporal-Difference Learning[C]//Canadian Conference on Artificial Intelligence. Springer, Cham, 2017: 3-14.

[7] Silver D, Online Course: Introduction to Reinforcement Learning, Lecture 1. ppt: page 22.

## Declaration
Notice that I am also a beginner in Reinforcement Learining community, so if there is any mistake in my posts feel free to point out in the comment or contact me. Republishing the posts is also welcomed but please do include the source. Thank you!
