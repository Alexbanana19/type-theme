---
layout: post
title:  "Introduction to Deep Learning & Deep Belief Nets" by Geoffrey Hinton
feature-img: "img/sample_feature_img.png"
comments: true
---
## Histroy

### Backpropagation

- didn't work in early times:
  - too slow
  - didn't work in rnn

- didn't have to use smooth no-linearity

- what's wrong:
  - labeled data
  - learning time does not scale well
  - get stuck on poor local minima

### Major issues in DL

- Deep vs Shallow

- Supervised vs Unsupervised

- Prior and Prejudice
    - how much prior knoledge should we try to wire into the weights or the activities of the networks?

    - smuggling knoledge into the networks without prejudicing the solution by manipulating the trainning set.

    - don't mess up with the weights in a network but the trainning data

- How do we map a task onto a neural network

  - attention and recursion

    - what human do vision: decide where __not__ to look at.


  - intelligent decomposition vs brute force scanning

    - vision is about sampling, but this is not happening in computer vision

### Overcoming the limitations of Backpropagation by using unsupervised learning

- keep the efficiency an simplicity of using a gradient method.

- adjust the weights to maximize the probability that a generative emodel would have produced the sensory input.

- learn P(image) but not P(label | image)


## Belief nets

- a belief net is a directed acyclic graph composed of stochastic variables

- solve two problems:
  - inference: infer the states of the unobserved variables

  - learning: adjust the intersatctins between variables to make the network more likely ot generate the observed data

- stohastic  binary units
  - thses have a state of 1 or 0 (bernoulli)

  - the probability of turning on is determined by the weighted input from other units(plus a bias)

- problems in dbn:
    - it is easy to generate an unbiased example at the leaf nodes

    - it is hard to infer the posterior distribution over all possible configurations of hidden causes

    - it is hard to even get a sample from the posterior

- learning rule for sigmoid belief net

  - learning is easy if we can get an unbiased sample from the posterior distribution over hidden states given the observed data.

  - for each unit, maximize the log probability that its binary state in the sample from the posterior would be generated by the sampled binary states or its parents

- why it is hard to learn sigmoid bn one layer at a time  
  - the posterior is typically complicated because of "explaining away"

  - the posterior depends on the prior as well as the likelihood

  - we need to integrate over all possible configurations of the higher variables to get the prior for the firsr hidden layer

- solution:
  - monte carlo methods: sample from the posterior

  - variational methods
      - approximate samples from the posterior

      - learing is guaranteed to improve a variational log probability of generating the observed data

## A breakthrough that makes deep learning efficient

- we connect binary stochastic neurons using symmetric connections we get a Boltzmann machine


### Restricited Boltzmann Machines

- we restrict the connectivity to make learning easier

- in an RBM, the hidden units are conditionally independent given the visible states.
  - so we can quickly get an unbiased sample from the posterior distribution when given a data-vector

  - this is a big advantage ober directed belief net

## the energy of a joint configurations

- ignoring terms to do with biases

- weights->energies->probability

-

## three ways to combine probability density models(an underlying theme of the tutorial)

- Mixture: weak, never gives sharp knowledge

- Product: give sharp knowledge

- composition: use the values of the latent variables of one model as the data for the next models
  - works well for learning multiple layers of representation, but only if the individual models are undirected

## the main reason RBM is interesting

- first train a layer of features that receive input directly from the pixels

- then treat the activations of the trained features as if they were pixels and learn features of features in a second hidden layer

- it can be proved that each time we add another layer of features we improve a variational lower bound on the log probability of the trainning data

## averaging factorial distributions

## equivalence between RBM's and directed networks
